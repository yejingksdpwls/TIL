- 사전 설정
    - *단어 사전 구축 함수*
    
    ```python
    # 단어 사전 구축 함수 정의 (리뷰 텍스트를 토큰화하고 사전 생성)
    def build_vocab(reviews):
        tokenized_reviews = [word_tokenize(review) for review in reviews]
        vocab = build_vocab_from_iterator(tokenized_reviews, specials=["<pad>", "<unk>"])
        vocab.set_default_index(vocab["<unk>"])  # 알 수 없는 단어 처리
        return vocab
    ```
    
    → `[word_tokenize(review) for review in reviews]` : 리뷰 데이터 토큰화
    
    → `build_vocab_from_iterator(tokenized_reviews, specials=["<pad>", "<unk>"])` : 토큰화된 모든 단어 고유하게 인덱싱하여 단어 사전을 만들고, 특별 토큰인 `<pad>`와 `<unk>`도 포함해 모델이 패딩이나 미등록 단어를 처리할 수 있게 했습니다.
    
    → `vocab.set_default_index(vocab["<unk>"])` : 모델에 없는 단어가 입력되었을 때 자동으로 `<unk>` 토큰의 인덱스 반환해, 미등록 단어 처리시 오류 없이 모델 예측
    
    - *데이터셋 클래스 정의*
    
    ```python
    # 데이터셋 클래스 정의 (데이터셋을 커스텀하여 사용하기 위해 클래스 정의)
    class ReviewDataset(Dataset):
        def __init__(self, reviews, ratings, text_pipeline, label_pipeline):
            self.reviews = reviews
            self.ratings = ratings
            self.text_pipeline = text_pipeline
            self.label_pipeline = label_pipeline
    
        def __len__(self):
            return len(self.reviews)
    
        def __getitem__(self, idx):
            review = self.text_pipeline(self.reviews[idx])
            rating = self.label_pipeline(self.ratings[idx])
            return review, rating
    ```
    
    →  모델 학습을 위해 DataLoader에 연결해 배치 단위로 데이터 불러올 때 사용하기 위한 클래스를 정의했습니다.
    
    - *패딩 추가*
    
    ```python
    # 패딩과 텐서 변환을 위한 collate 함수 정의
    def collate_fn(batch):
        reviews, ratings = zip(*batch)  # 배치에서 리뷰와 레이팅 분리
        reviews_padded = pad_sequence(reviews, batch_first=True)  # 리뷰를 패딩
        ratings_tensor = torch.tensor(ratings, dtype=torch.long)  # 레이팅을 텐서로 변환
        return reviews_padded, ratings_tensor  # 패딩된 리뷰와 레이팅 반환
    ```
    
    → 길이가 다른 시퀀스들을 가장 긴 시퀀스에 맞춰 패딩하여 같은 길이로 맞추어, 텐서로 변환해 모델에 전달하도록 함수를 정의했습니다.
    
    - *데이터 분할*
    
    ```python
    # 데이터 분할
    # 훈련 데이터와 임시 데이터(검증 + 테스트 데이터) 분리(train_test_split 사용)
    train_reviews, temp_reviews, train_ratings, temp_ratings = train_test_split(reviews, ratings, test_size=0.3, random_state=42)
    
    # 임시 데이터를 검증 데이터와 테스트 데이터로 분리(train_test_split 사용)
    test_reviews, rtest_reviews, test_ratings, rtest_ratings = train_test_split(temp_reviews, temp_ratings, test_size=0.5, random_state=42)
    
    # 인덱스 초기화 (데이터 프레임 인덱스 리셋)
    train_reviews.reset_index(drop=True, inplace=True)
    train_ratings.reset_index(drop=True, inplace=True)
    test_reviews.reset_index(drop=True, inplace=True)
    test_ratings.reset_index(drop=True, inplace=True)
    rtest_reviews.reset_index(drop=True, inplace=True)
    rtest_ratings.reset_index(drop=True, inplace=True)
    ```
    
    → **조기종료**를 위해 훈련, 검증, 테스트 데이터로 분리
    
- LSTM 모델
    - *모델 정의*
    
    ```python
    # 모델 정의 (LSTM 모델 구조를 설정하여 시퀀스 처리 수행)
    class LSTMModel(nn.Module):
      def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):
        super(LSTMModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Sequential(
        nn.Linear(hidden_dim, 128),
        nn.ReLU(),
        nn.Dropout(0.5),
        nn.Linear(128, output_dim)
    )
    
      # 순전파 정의 (입력 텍스트를 임베딩하고 LSTM과 FC 계층에 통과)
      def forward(self, text):
        embedded = self.embedding(text)
        output, (hidden, cell) = self.lstm(embedded)  # 배치 사이즈에 맞게 차원 추가
        return self.fc(hidden[-1].view(hidden.size(1), -1))
    ```
    
    → `self.fc = nn.Sequential(
        nn.Linear(hidden_dim, 128),
        nn.ReLU(),
        nn.Dropout(0.5),
        nn.Linear(128, output_dim)
    )` : *’완전 연결 계층’* 으로 구성된 출력층으로 은닉 상태에서 나오는 출력층을 정제 해 최종 예측값을 생성하도록 설정했습니다. 
    
    → 여기서는 **ReLu** 의 비선형 함수를 사용하였습니다. 
    
    → Dropout을 이용해 50%의 노드를 비활성화해 과적합을 방지했습니다.
    
    → 
      `def forward(self, text):
        embedded = self.embedding(text)
        output, (hidden, cell) = self.lstm(embedded)  # 배치 사이즈에 맞게 차원 추가
        return self.fc(hidden[-1].view(hidden.size(1), -1))`
    
    → **순전파 메서드**로 입력 텍스트 시퀀스를 LSTM으로 처리해 최종적으로 각 시퀀스의 예측값을 계산해 모델의 출력으로 제공했습니다.
    
    - *손실 함수, 옵티마이저 정의*
    
    ```python
    # 손실 함수와 옵티마이저 정의 (교차 엔트로피 손실과 Adam 사용, 학습률 0.01로 설정)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    ```
    
    → `Adam` : SGD 개선한 방법 → 학습률을 각 파라미터에 맞춰 동적으로 조정해 빠르고 안정적으로 학습하도록 합니다.
    
    - *조기 종료 함수*
    
    ```
    # 조기 종료 함수 정의
    class EarlyStopping:
        def __init__(self, patience=3, verbose=False):
            self.patience = patience
            self.verbose = verbose
            self.best_loss = None
            self.counter = 0
    
        def __call__(self, val_loss):
            if self.best_loss is None:
                self.best_loss = val_loss
            elif val_loss < self.best_loss:
                self.best_loss = val_loss
                self.counter = 0
            else:
                self.counter += 1
                if self.counter >= self.patience:
                    if self.verbose:
                        print("Early stopping triggered.")
                    return True
            return False
    ```
    
    → 새로 계산된 검증 손실이 기존의 손실보다 작지 않게 되는 현상이 patience 만큼 반복할 시 학습을 조기에 종료해 불필요한 에포크를 줄이고 과적합을 방지합니다.