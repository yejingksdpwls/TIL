# [ 딥러닝 ]

![](https://velog.velcdn.com/images/yejingksdpwls/post/d0454dff-7bd7-4784-ab65-ef6810e43832/image.png)

- **인공신경망**을 기반으로 하는 **기계 학습** 중 하나
- 다층 신경망을 통해 특징 자동으로 학습
- 특징
    - 비선형 추론
    - 다층 구조 (고차원 특징 학습)
    - 특징 자동 추출
&nbsp;
### [ 퍼셉트론 ]

- **단일 퍼셉트론**
    - *퍼셉트론* : 인공신경망의 가장 기본적 단위로 하나의 뉴런을 모델링 한 것
    
    → 입력 값을 받아 가중치를 곱하고, 그 값을 모두 더한 뒤 *활성화 함수*를 통해 출력값을 결정함
    

![](https://velog.velcdn.com/images/yejingksdpwls/post/cc7d431c-57a1-48d9-8437-564066fac6dd/image.png)

&nbsp;
- **다층 퍼셉트론 (MLP)**
    
    : 여러 층의 퍼셉트론을 쌓아 올린 신경망 구조
    
    - 입력층, 은닉층, 출력층으로 구성되어 각 층의 뉴런들이 서로 연결
    
    ![](https://velog.velcdn.com/images/yejingksdpwls/post/8283d765-dde8-4306-a978-fc5b4c8016fe/image.png)

    

→ *입력층* : 외부 데이터가 입력되는 부분

( 뉴런 수 : 입력 데이터의 특징 수 )

→ *은닉층* : 입력층과 출력층 사이에서 입력된 데이터 처리, 특징 추출하는 역할

( 은닉층의 뉴런 수/층 수는 모델의 복잡성/성능에 영향을 미침 )

→ *출력층* : 최종 예측 값 출력

( 출력층 뉴런 수 = 예측 클래스 or 회귀 문제 출력 차원 )
&nbsp;
- **단일 퍼셉트론의 한계**
    - **선형 분류기**이므로 XOR 문제와 같은 비선형 문제 해결 불가능
    
    ( **XOR 문제** : 두 입력 값이 다를 때만 1 출력 )
    
    —> **MLP** : *은닉층*을 통해 *비선형성* 학습 가능
    &nbsp;
- **활성화 함수**
    - **필요성** :  비선형성을 도입해 신경망이 복잡한 패턴 학습할 수 있게 함
    - *종류*
        - **ReLu**
            
            ![](https://velog.velcdn.com/images/yejingksdpwls/post/884c5f3f-1b67-4c37-9abc-da1b32014f26/image.png)

            
            - 장점 : 계산 간단, 기울기 소실 문제 완화
            - 단점 : 음수 입력에 대해 기울기가 0이 되어 *’죽은 ReLu’* 문제 발생 가능
            &nbsp;
        - **시그모이드 (Sigmoid)**
            
            ![](https://velog.velcdn.com/images/yejingksdpwls/post/01d7f080-6269-42c4-bbfd-1fcbeec7874b/image.png)

            
            - 장점 : 출력값 0~1로 제한되어 중심이 0에 가까워짐
            - 단점 : 기울기 소실 문제 발생 가능, 출력값 0 or 1에 가까워질 때 학습이 느려지는 문제 발생
            &nbsp;
        - **Tanh**
            
            ![](https://velog.velcdn.com/images/yejingksdpwls/post/fe828337-8b43-4a7f-8891-ce476f6479e4/image.png)

            
            - 장점 : 출력값 -1 ~ 1로 제한되어 중심 0에 가까워짐
            - 단점 : 기울기 소실 문제 발생 가능
&nbsp;
- **손실함수**
    - 모델 성능 평가, *최적화 알고리즘*을 통해 모델 학습시키는데 사용됨
    &nbsp;
    - **MSE**
        
        ![](https://velog.velcdn.com/images/yejingksdpwls/post/cfa8734a-0f73-4125-924d-d16e7ef6bfbb/image.png)

        
        - *회귀* 문제에 주로 사용
        &nbsp;
    - **Cross-Entropy**
        
        ![](https://velog.velcdn.com/images/yejingksdpwls/post/8792dc73-b2a4-489a-a32e-8215c717dbae/image.png)

        
        - *분류* 문제에 주로 사용
    
- *최적화 알고리즘* : **손실함수를 최소화**하기 위해 모델 가중치 조정하는 방법
&nbsp;
### [ 역전파 ] ****

- 신경망의 가중치를 학습시키기 위해 사용되는 알고리즘
- *과정*
    - 입력 데이터를 통해 예측 값 계산
    - 예측 값과 실제 값 간의 차이를 손실 함수를 통해 계산
    - 손실 함수의 기울기를 출력층에서 입력층 방향으로 계산
    - 최적화 알고리즘을 통해 가중치 업데이트
&nbsp;
## [ 신경망 모델 ]

### [ 인공 신경망 (ANN) ]

- 생물학적 신경망 모방
- 입력층, 은닉층, 출력층으로 구성
    
    ( 각 층은 뉴런으로 이루어짐 )
    
- **동작 방식**
    - 순전파
    - 손실 계산
    - 역전파
    
    ![](https://velog.velcdn.com/images/yejingksdpwls/post/cbde951f-c747-4a46-ac59-bf5a4b2b34a2/image.png)

    
- **출력 레이어 유형 및 활용**
    - 회귀 문제
        - 출력층 뉴런 수 = 예측하려는 연속적인 값의 차원
        - 활성화 함수 : **선형 함수**
    - 이진 분류 문제
        - 출력층 뉴런 수 = 1
        - 활성화 함수 : **시그모이드 함수**
            
            ( 출력값 : 0~1 사이의 확률 )
            
    - 다중 분류 문제
        - 출력층 뉴런 수 : 예측하려는 클래스 수
        - 활성화 함수 : **소프트 맥스**
    - 코드
    
    ```python
    # 필요한 라이브러리 임포트
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torchvision
    import torchvision.transforms as transforms
    
    # 데이터 로드 및 전처리
    # 데이터셋 전처리
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    
    # MNIST 데이터셋 로드
    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)
    
    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)
    
    # ANN 모델 정의
    class SimpleANN(nn.Module):
        def __init__(self):
            super(SimpleANN, self).__init__()
            self.fc1 = nn.Linear(28 * 28, 128)  # 입력층에서 은닉층으로
            self.fc2 = nn.Linear(128, 64)       # 은닉층에서 은닉층으로
            self.fc3 = nn.Linear(64, 10)        # 은닉층에서 출력층으로
    
        def forward(self, x):
            x = x.view(-1, 28 * 28)  # 입력 이미지를 1차원 벡터로 변환
            x = torch.relu(self.fc1(x))
            x = torch.relu(self.fc2(x))
            x = self.fc3(x)
            return x
    
    # 모델 학습 
    # 모델 초기화
    model = SimpleANN()
    
    # 손실 함수와 최적화 알고리즘 정의
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
    
    # 모델 학습
    for epoch in range(10):  # 10 에포크 동안 학습
        running_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data
    
            # 기울기 초기화
            optimizer.zero_grad()
    
            # 순전파 + 역전파 + 최적화
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
    
            # 손실 출력
            running_loss += loss.item()
            if i % 100 == 99:  # 매 100 미니배치마다 출력
                print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
                running_loss = 0.0
    
    print('Finished Training')
    
    # 모델 평가
    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')
    ```
    

→ **[ 상세 설명 ]**

- `nn.Linear(in_features, out_features)`
    - 선형 변환을 적용하는 완전 연결 레이어를 정의하는 역할
    - `in_features` : 입력 특징의 수
    - `out_features` : 출력 특징의 수
- `torch.relu` : ReLu 함수 적용
- `view` : 텐서 크기 변경
    - `x.view(-1, 28*28) : 입력 이미지 1차원 벡터로 변환
- `nn.CrossEntropyLoss` : 예측 값과 실제 값 간의 *교차 엔트로피 손실* 계산
    
    ( 다중 분류 문제에서 주로 사용 )
    
- `optim.SGD(lr, momentum)` : **확률적 경사 하강법** 알고리즘 적용
    - `lr` : 학습률
    - `momentum` : 기울기 반영률 ( 모멘텀 )
- `optimizer.zero_grad()` : 기울기 초기화
- `loss.backward()` : 역전파 알고리즘 적용
- `optimizer.step()` : 계산된 기울기를 바탕으로 가중치 업데이트
- `torch.no_grad()` : 평가 단계에서 불필요한 기울기 계산 비활성화
- `torch.max` : 텐서의 최대값 찾는 역할
&nbsp;
### [ 합성곱 신경망 (CNN) ]

![](https://velog.velcdn.com/images/yejingksdpwls/post/258a4f51-9564-4741-acfe-89f945479b1b/image.png)


- 기본 구조
    - **이미지와 같은 2차원** 데이터 특징을 효과적으로 추출하기 위해 설계된 신경망
    - **합성곱 층, 폴링 층, 완전 연결층**으로 구성

+) ANN과 달리 *동시에* 보는 특성이 있어 **서로 간의 관계**를 잘 볼 수 있음
&nbsp;
- **합성 곱 연산 원리**
    
    ![](https://velog.velcdn.com/images/yejingksdpwls/post/9564afdd-42a7-4f20-af5c-b92f86f30391/image.png)

    
- 필터를 이미지의 각 위치에 슬라이딩 해, 필터와 이미지의 해당 부분간의 점 곱을 계산함
- 계산된 값은 특정 맵의 해당 위치에 저장됨
&nbsp;
- **필터의 역할**

![](https://velog.velcdn.com/images/yejingksdpwls/post/c79eca94-04ad-4b68-b4e6-2a0e98f8030c/image.png)


- 다양한 국소적 패턴 ( ex. edge, corner, texture … ) 학습
- 여러 개의 필터를 통해 다양한 특징 맵 생성 가능
&nbsp;
- **풀링레이어**
    
    ![](https://velog.velcdn.com/images/yejingksdpwls/post/478ba076-8f46-427f-ab05-ee8b7a3878ab/image.png)

    
    - 특정 맵의 크기를 줄이고, 중요한 특징을 추출하는 역할
    - *종류*
        - Max Pooling
            - 필터 크기 내에서 최대 값 선택
            - 중요한 특징 강조, 불필요한 정보 제거
        - Average Pooling
            - 필터 크기 내에서 평균 값 계산
            - 특징 맵 크기 줄이면서, 정보 손실 최소화
&nbsp;
- **플래튼 레이어**
    
    ![](https://velog.velcdn.com/images/yejingksdpwls/post/0478a070-bddb-4fe9-b9b6-0f27e65de954/image.png)

    
    - 2차원 특징 맵을 1차원으로 변환하는 역할
    - 완전 연결 층에 입력으로 사용하기 위해 필요
    - 특징 맵은 이미지에서 축약된 정보들이 들어있으므로 **ANN** 모델에도 적용 가능해짐
&nbsp;
- **가중치 업데이트**
    - 손실 함수 계산 → 기울기 계산 → 가중치 업데이트
    - **CNN 가중치** : “필터 내부의 값”으로 학습

- 실습

```python
# 간단한 CNN 모델 정의

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)  # 입력 채널 3, 출력 채널 32, 커널 크기 3x3
        self.pool = nn.MaxPool2d(2, 2)               # 풀링 크기 2x2
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 입력 채널 32, 출력 채널 64, 커널 크기 3x3
        self.fc1 = nn.Linear(64 * 8 * 8, 512)        # 완전 연결 층
        self.fc2 = nn.Linear(512, 10)                # 출력 층 (10개의 클래스)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 64 * 8 * 8)  # 플래튼
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x
```

- `nn.Conv2d`: 2차원 합성곱 층을 정의
- `nn.MaxPool2d`: 2차원 최대 풀링 층을 정의
- `view`: 텐서의 크기를 변경

```python
 
 # 모델 학습
 # 모델 초기화
model = SimpleCNN()

# 손실 함수와 최적화 알고리즘 정의
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# 모델 학습
for epoch in range(10):  # 10 에포크 동안 학습
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data

        # 기울기 초기화
        optimizer.zero_grad()

        # 순전파 + 역전파 + 최적화
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # 손실 출력
        running_loss += loss.item()
        if i % 100 == 99:  # 매 100 미니배치마다 출력
            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0

print('Finished Training')
```

- `nn.CrossEntropyLoss`: 다중 클래스 분류 문제에서 주로 사용되는 손실 함수
    - 예측 값과 실제 값 간의 교차 엔트로피 손실 계산
- `optim.SGD`: 확률적 경사 하강법 최적화 알고리즘을 정의
- `optimizer.zero_grad()`: 이전 단계에서 계산된 기울기를 초기화
- `loss.backward()`: 역전파를 통해 기울기를 계산
- `optimizer.step()`: 계산된 기울기를 바탕으로 가중치 업데이트

```python

# 모델 평가
correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')
```

- `torch.no_grad()`: 평가 단계에서는 기울기를 계산할 필요가 없으므로, 이를 비활성화하여 메모리 사용을 줄임
- `torch.max`: 텐서의 최대 값을 찾는 함수
- `labels.size(0)`: 배치 크기를 반환
- `(predicted == labels).sum().item()`: 예측 값과 실제 값이 일치하는 샘플 수 계산
&nbsp;
### [ 순환 신경망 (RNN) ]

![](https://velog.velcdn.com/images/yejingksdpwls/post/cb11eef2-9e71-4954-90e5-90ad182bdab7/image.png)


- 기본 구조
    - 시계열 데이터나 순차적인 데이터를 처리하기 위해 설계된 신경망
    - **이전 시간 단계의 정보를 현재 시간 단계로 전달**해 시퀀스 데이터의 패턴 학습 가능

- *동작 원리*
    - 입력 데이터의 이전 시간 단계의 은닉 상채를 입력으로  받아, **현재 시간 단계의 은닉 상태** 출력
    - 은닉 상태는 시퀀스의 정보 저장하고 다음 시간 단계로 전달됨
    - 시퀀스의 각 시간 단계에서 동일한 가중치 공유해 시퀀스 패턴 파악
    - **순전파와 역전파**를 통해 가중치 학습

→ 가장 유용하지 않은 데이터를 소실해야 하는데 RNN 모델은 **가장 오래된 데이터를 소실**하는 문제점이 있음
&nbsp;
### [ RNN : LSTM ]

![](https://velog.velcdn.com/images/yejingksdpwls/post/7550019f-bbd2-4c71-b35a-a596d865285c/image.png)


- *셀 상태와 게이트 구조* 도입, **장기 의존성** 효과적으로 학습 가능
- *입력 게이트, 출력 게이트, 망각 게이트*를 사용해 정보 조절

→ 새로운 정보가 셀에 얼마나 반영될 수 있는지, 어떤 데이터가 삭제되어야 하는지, 어떤 데이터를 앞으로 셀에 전달할 지 **게이트**를 통해 결정
&nbsp;
### [ RNN : GRU ]

![](https://velog.velcdn.com/images/yejingksdpwls/post/996b26e3-11ca-4718-8aee-e13c6f020a93/image.png)


- LSTM의 변형, 셀 상태 대신 **은닉 상태** 사용하여 구조 단순화 함
- *업데이트 게이트와 리셋 게이트* 사용해 정보 조절
&nbsp;
—> **차이점**

![](https://velog.velcdn.com/images/yejingksdpwls/post/9f3bb095-ef8c-48fb-9fa7-765e7ee02be2/image.png)


- LSTM : **셀 상태와 은닉 상태 모두 사용**, 더 복잡한 게이트 구조
- GRU : **은닉 상태만** 사용, 더 간단한 게이트 구조

- 실습

```python
# 간단한 RNN 모델 정의
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleRNN, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), hidden_size)  # 초기 은닉 상태
        out, _ = self.rnn(x, h0)
        out = self.fc(out[:, -1, :])  # 마지막 시간 단계의 출력
        return out

input_size = 1
hidden_size = 32
output_size = 1
model = SimpleRNN(input_size, hidden_size, output_size)
```

- `nn.RNN`: 순환 신경망(RNN) 층을 정의
- `nn.Linear`: 선형 변환을 적용하는 완전 연결(fully connected) 레이어를 정의

```python
# 간단한 LSTM 모델 정의
class SimpleLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), hidden_size)  # 초기 은닉 상태
        c0 = torch.zeros(1, x.size(0), hidden_size)  # 초기 셀 상태
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])  # 마지막 시간 단계의 출력
        return out

model = SimpleLSTM(input_size, hidden_size, output_size)
```

- `nn.LSTM`: 장단기 메모리(LSTM) 층을 정의

```python
# 모델 학습
# 손실 함수와 최적화 알고리즘 정의
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# 모델 학습
num_epochs = 100
for epoch in range(num_epochs):
    outputs = model(X)
    optimizer.zero_grad()
    loss = criterion(outputs, y)
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')

print('Finished Training')
```

- `nn.MSELoss`: 평균 제곱 오차(MSE) 손실 함수를 정의
- `optim.Adam`: Adam 최적화 알고리즘을 정의합니다. `lr`은 학습률을 지정
- `optimizer.zero_grad()`: 이전 단계에서 계산된 기울기를 초기화
- `loss.backward()`: 역전파를 통해 기울기를 계산
- `optimizer.step()`: 계산된 기울기를 바탕으로 가중치를 업데이트

```python
# 모델 평가 및 시각화
# 모델 평가
model.eval()
with torch.no_grad():
    predicted = model(X).detach().numpy()

# 시각화
plt.figure(figsize=(10, 5))
plt.plot(y.numpy().flatten(), label='True')
plt.plot(predicted.flatten(), label='Predicted')
plt.legend()
plt.show()
```

- `model.eval()`: 모델을 평가 모드로 전환
- `torch.no_grad()`: 평가 단계에서는 기울기를 계산할 필요가 없으므로, 이를 비활성화하여 메모리 사용을 줄임
- `detach()`: 텐서를 계산 그래프에서 분리