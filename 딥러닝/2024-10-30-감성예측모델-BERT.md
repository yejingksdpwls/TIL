## [ 영화 리뷰 감성 예측 모델 구현 및 평가 ]
&nbsp;
&nbsp;
오늘 진행한 코드는 **영화 리뷰 데이터셋**을 대상으로 두 가지 감성 예측 모델을 구성하고 학습해 보았습니다.
&nbsp;
&nbsp;
### [ 1. 텍스트 전처리와 이진 레이블 변환 ]
```python
import re

# 전처리 함수
def preprocess_text(text):
    if isinstance(text, float):
        return ""
    text = text.lower()  # 대문자를 소문자로
    text = re.sub(r'[^\w\s]', '', text)  # 구두점 제거
    text = re.sub(r'\d+', '', text)  # 숫자 제거
    text = text.strip()  # 앞뒤 공백 제거
    return text
```

-> `preprocess_text()` : 텍스트에서 불필요한 구두점과 숫자를 제거하고 소문자로 변환했습니다.
```python
# 리뷰 평점 기반 이진 레이블 변환
df['sentiment'] = df['score'].apply(lambda x: 1 if x >= 3 else 0)
```
-> 리뷰 평점(score)을 기준으로 평점을 감성 레이블(긍정, 부정)로 변환했습니다 (3점 이상은 긍정, 미만은 부정)
&nbsp;
&nbsp;
### [ 2. BERT 기반 감성 분석 모델 ]
-> **BERT** 모델은 '사전 학습된 언어 모델'을 이용해 더 많은 문맥 정보를 활용 예측 성능을 높입니다.
```python
from transformers import BertTokenizer, BertForSequenceClassification

# 토크나이저와 모델 준비
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
```
-> `BertTokenizer` : 리뷰 데이터를 토크나이즈하고, 리뷰 감성을 학습시키기 위해 BertForSequenceClassification 모델을 사용했습니다.
```python
# 데이터셋 클래스 정의
class MovieReviewDataset(torch.utils.data.Dataset):
    def __init__(self, texts, labels, tokenizer, max_length):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long),
        }
```
-> `MovieReviewDataset 클래스` : 토큰화된 텍스트와 라벨을 포함하는 데이터셋을 생성해 BERT 모델에 적용했습니다.
&nbsp;
&nbsp;
### [ 배운 점 ]
1 . **텍스트 전처리와 이진 레이블링**
: 텍스트 데이터에서 감성 분석 시 이진 레이블이 단순하면서도 예측 정확도 평가에 유리한 것을 확인했습니다.
&nbsp;
2. **LSTM과 BERT의 차이점**
: LSTM은 임베딩 계층부터 차례로 감성 예측을 수행하고, BERT는 사전 학습된 언어 모델을 통해 더 많은 문맥 정보를 활용해 예측 성능을 높였습니다.
&nbsp;
3. **모델 평가 및 예측 함수의 중요성**
: 모델 훈련과 평가의 지표를 설정하고 예측 함수로 실제 입력 데이터를 테스트하는 과정을 통해 모델의 실용성을 높이는 방법을 배웠습니다.
이 코드와 결과는 텍스트 데이터의 감성 분석과 BERT 사용법을 익히는 데 큰 도움이 되었습니다.
&nbsp;
4. **팀 내 패키지 사전 설정 중요성**
: 개인으로 하는 과제가 아닌 팀으로 진행하는 과제를 안정적으로 수행하기 위해서는, 본격적인 시작에 앞서 기본적으로 사용할 패키지의 버전 등의 정보들을 미리 설정하고 진행해야 안정적으로 취합이 가능하다는 것을 깨달았습니다.



