오늘은 **API 사용**을 통해 텍스트 생성, 음성 합성, 컴퓨터 비전 등의 기능을 간단히 구현할 수 있었고, **사전 학습 모델**을 활용해 파이프라인을 사용하여 텍스트 생성, 감성 분석, 단어 임베딩 등을 수행했습니다. 이를 통해 복잡한 AI 모델을 손쉽게 사용할 수 있는 장점과 사전 학습된 모델의 한계에 대해서도 살펴보았습니다.

## [ API로 인공지능 활용 ]

- **API**
    
    개발자가 복잡한 AI 기능을 손쉽게 사용할 수 있도록 제공되는 인터페이스
    
    → 서버와 클라이언트 간 요청과 응답 주고받는 방식으로 작동
    

- 활용 방법
    - 텍스트 생성 API : *ChatGPT*
        - **GPT**라는 언어 모델을 기반으로, 사용자가 입력한 텍스트에 대해 자연스럽고 유창한 응답 생성
    - 음성 합성 API : *ElevenLabs*
        - 텍스트를 인간의 목소리처럼 자연스럽게 읽어주는 기능
    - 컴퓨터 비전 API : *Google Vision AI*
    - 음성 인식 API : *Google Cloud Speech-to-Text*
    - 번역 API : *DeepL*

- API 활용 장점
    - 손쉬운 사용
    - 신속한 개발
    - 확장성 (복합적 기능 구현 가능)
- API 활용 단점
    - 비용의 위험
    - 제한된 제어
    - 높은 의존성
&nbsp;
&nbsp;
### [ 사전 학습 모델 활용 ]

```python
model = Transformer(d_model = 512, nhead = 8, num_encoder_layers = 6, num_decoder_layers = 6)
# d_model : 단어의 임베딩 차원 수
# nhead : multi head attention의 헤드 수
# ~_layers : 인코더/디코더 형성 레이어 수
```

→  `transformer` 모델 정의

```python
# 옵티마이저 설정
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
# transformer 처럼 파라미터가 많고 깊고 복잡한 모델의 경우 학습률을 작게 설정하는 편
criterion = nn.CrossEntropyLoss()
```

→ `Adam`, `CrossEntropyLoss` 사용

```python
# 모델 학습 단계 (사전 학습된 모델 활용)
model = torch.hub.load('huggingface/pytorch-transformers', 'modelForCausalLM', 'gpt2')
tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'gpt2')
```

→ `huggingface`  사전 학습된 모델 활용한 모델 학습 단계

```python
input_text = 'Once upon a time'
input_idx = tokenizer.encode(input_text, return_tensors='pt')
output = model.generate(input_idx, max_length=50, num_return_sequences = 1)
# 인간이 알아볼 수 있는 형태도 다시 해석
tokenizer.decode(output[0], skip_special_tokens = True)
```

→ 결과 : 
![](https://velog.velcdn.com/images/yejingksdpwls/post/ea18910e-be1c-4a16-abd0-d693c11dd808/image.png)


→ 전과 같이 문장 토큰화 후 문장 생성한 뒤, 디코더를 통해 인간이 알아볼 수 있는 형태로 다시 해석 후 출력

- 문제점
    - 대형 모델의 학습이 어려움
        - 데이터 및 컴퓨팅 자원의 한계
        - 모델 크기와 메모리 사용량
    - 복잡한 모델 직접 구현 어려움
        - 구현의 어려움
        - 하이퍼파라미터 튜닝
    - 사전 학습 모델 활용의 한계
        - 어려운 맞춤화
        - 비용의 문제
&nbsp;
&nbsp;
&nbsp;
### [ GPT2 모델 실습 ]

```python
!pip install transformers==4.37.0
```

→ `GPT2` 모델을 사용하기 위해서는 transformers 버전을 낮춰줘야 함

```python
from transformers import pipeline

generator = pipeline('text-generation', model='gpt2')

result = generator('do you know', max_length=50, num_return_sequences=2)
print(result)
```

→ `max_lenght` 가 클 수록 긴 문장을 생성하기는 하지만, 항상 max 길이만큼 생성하는 것은 아니다.

→ **무작위성**을 가지고 있어, 실행할 때마다 다른 결과가 나옴을 확인할 수 있음
&nbsp;
&nbsp;

### [ 감성 분석 모델 실습 ]

```python
sentiment_analysis = pipeline('sentiment-analysis') 
result = sentiment_analysis('i have sympathy about you. But i don\'t know')
print(result) 
```

→ **파이프라인** : 작업/모델을 지정해 줄 수 있음


### [ BERT 모델 실습 ]

```python
sentiment_analysis = pipeline('sentiment-analysis', model='roberta-base') 
result = sentiment_analysis('i love him. But he hate me. I\m so sad')
print(result) 
```

→ **제대로 작동 안 함**

→  BERT 모델은 일반적인 언어 모델링 작업에 대해 사전 학습된 모델이지만, 특정 작업에 바로 사용할 수 있는 **가중치가 없기 때문** 
→  **파인튜닝**이 필요함
→ 파인튜닝을 통해 분류 레이어 가중치 학습시키지 않으면, 모델이 적절한 예측 못 할 가능성 큼
&nbsp;&nbsp;
### [ Word2Vec 모델 실습 ]

```python
# Word2Vec 모델
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess
from scipy.spatial.distance import cosine

# 문장 단어 단위로 쪼개기
sentences = ['I love him very much',
             'But he doesn\'t love me',
             'That team A is very powerful',
             'But I think team name B will be win']

processed = [simple_preprocess(sentence) for sentence in sentences]  
print(processed)

# 모델 정의
model = Word2Vec(sentences = processed, vector_size = 5, window = 5, min_count = 1, sg = 0)
```

→ `vector_size`  : 각 단어를 몇 차원 공간에 매핑할 것인지 설정
→ `window_size`  : 주변 단어와의 관계를 얼마나 넓게 볼 것인지 설정
→ `min_count`  : 최소 몇 번 등장해야 해당 단어를 임베딩 할 것인지 설정
→ `sg`  : 어떤 알고리즘을 통해 학습할 것인지 설정

```python
love = model.wv['he']
team = model.wv['team']

# 코사인 유사도 계산
sim = 1-cosine(he, team)
print(sim)
```
&nbsp;
&nbsp;
### [ BERT 기반 임베딩 모델 실습 ]

```python
# bert 기반의 임베딩 모델
from transformers import BertModel, BertTokenizer
import torch
from scipy.spatial.distance import cosine

model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

sentences = ['I love him very much',
             'But he doesn\'t love me']

input1 = tokenizer(sentences[0], return_tensors = 'pt')
input2 = tokenizer(sentences[1], return_tensors = 'pt')

with torch.no_grad():
  output1 = model(**input1)
  output2 = model(**input2)

embedding1 = output1.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()
embedding2 = output2.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()

sim = 1-cosine(embedding1, embedding2)

print(sim)
```
&nbsp;
### [ 배운 점 ]
* **API**를 통해 복잡한 기능을 쉽게 사용할 수 있지만, 비용과 의존성 문제가 발생할 수 있다는 점을 알게 되었다.

* **사전 학습 모델**은 기본적으로 학습된 상태로 다양한 작업에 활용 가능하나, 특정 작업 성능을 높이기 위해 파인튜닝이 필요할 수 있다.

* **텍스트 임베딩**을 위해 Word2Vec과 BERT 임베딩을 실습해 봤으며, 이를 통해 문장 간 유사도를 코사인 유사도로 계산하는 방법을 익혔다.