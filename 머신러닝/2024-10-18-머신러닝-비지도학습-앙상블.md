## [ 앙상블 학습 ]

- 여러 개의 학습 모델 결합해 하나의 강력한 모델 만드는 기법
- 단일 모델보다 **더 높은 에측 성능, 일반화 능력**

### [ 배깅과 부스팅 ]

**[ 배깅 ]**

- 여러 개의 학습 모델 병렬로 학습
- 예측 결과 평균 or 다수결로 결합
- 데이터 샘플링 과정에서 **부트 스트랩** 기법 사용
    - **부트스트래핑** : 원본 데이터셋에서 중복 허용한 무작위 샘플 생성
- *장점*
    - 과적합 감소
    - 안정성 향상
    - 병렬 처리 가능
&nbsp;

**[ 부스팅 ]**

- 여러 개의 **약한 학습기**를 순차적으로 학습시켜, 결과들을 결합하여 **강한 학습기** 만듦
- 이전 모델이 잘못 예측한 데이터 포인트에 가중치 부여해 다음 모델이 더 잘 학습하도록 함
- *장점*
    - 높은 예측 성능
    - 과적합 방지 ( 모델 복잡도 조절 )
    - 순차적 학습 ( 이전 모델의 오류 보완 )
&nbsp;

- 코드

```python
# 배깅 모델 구현 및 평가
# 배깅 모델 생성
bagging_model = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=100, random_state=42)

# 모델 학습
bagging_model.fit(X_train_scaled, y_train)

# 예측
y_pred_bagging = bagging_model.predict(X_test_scaled)

# 평가
mse_bagging = mean_squared_error(y_test, y_pred_bagging)
print(f'배깅 모델의 MSE: {mse_bagging}')

# 부스팅 모델 구현 및 평가
from sklearn.ensemble import GradientBoostingRegressor

# 부스팅 모델 생성
boosting_model = GradientBoostingRegressor(n_estimators=100, random_state=42)

# 모델 학습
boosting_model.fit(X_train_scaled, y_train)

# 예측
y_pred_boosting = boosting_model.predict(X_test_scaled)

# 평가
mse_boosting = mean_squared_error(y_test, y_pred_boosting)
print(f'부스팅 모델의 MSE: {mse_boosting}')

```
&nbsp;
&nbsp;
### [ 랜덤 포레스트 ]

![](https://velog.velcdn.com/images/yejingksdpwls/post/bcc8c77e-2e0c-4bb9-a953-3f2ee96d2653/image.png)

- **배깅** 기법을 기반으로 하는 앙상블 모델
- 각 트리가 *독립적*으로 학습되어 **과적합 방지, 예측 성능 향상**
&nbsp;
- *원리*
    - 부트스트랩 샘플링
    - 결정 트리 학습 → 각 노드에서 무작위로 선택된 특성의 일부만 사용하여 분할 수행
    - 예측 결합 ( 회귀 : 평균, 분류 : 다수결 사용 )
- **무작위성 도입 → 다양성 증가, 과적합 방지**
    - 데이터 샘플링의 무작위성
    - 특성 선택의 무작위성
    &nbsp;
- 코드

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# 랜덤 포레스트 모델 생성
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# 모델 학습
rf_model.fit(X_train_scaled, y_train)

# 예측
y_pred_rf = rf_model.predict(X_test_scaled)

# 평가
mse_rf = mean_squared_error(y_test, y_pred_rf)
print(f'랜덤 포레스트 모델의 MSE: {mse_rf}')

## 중요 특성 확인
import matplotlib.pyplot as plt
import seaborn as sns

# 특성 중요도 추출
feature_importances = rf_model.feature_importances_

# 특성 중요도를 데이터프레임으로 변환
feature_importances_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
})

# 중요도 순으로 정렬
feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)

# 특성 중요도 시각화
plt.figure(figsize=(10, 7))
sns.barplot(x='Importance', y='Feature', data=feature_importances_df)
plt.title('Feature Importances in Random Forest')
plt.show()
```
&nbsp;
&nbsp;
### [ 그레디언트 부스팅 ]

- 여러 개의 **약한 학습기**를 순차적으로 학습시켜, 결과들을 결합하여 **강한 학습기** 만듦
- 이전 모델이 잘못 예측한 데이터 포인트에 가중치 부여해 다음 모델이 더 잘 학습하도록 함
- 각 트리가 *독립적*으로 학습되어 **과적합 방지, 예측 성능 향상**
&nbsp;
- 원리
    - 초기 모델 학습
    - 잔여 오차 계산 (예측 결과와 실제 값 간의 잔여 오차)
    - 잔여 오차 학습
    - 모델 업데이트
    - 반복
&nbsp;
- 코드

```python
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

# GBM 모델 생성
gbm_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# 모델 학습
gbm_model.fit(X_train_scaled, y_train)

# 예측
y_pred_gbm = gbm_model.predict(X_test_scaled)

# 평가
mse_gbm = mean_squared_error(y_test, y_pred_gbm)
print(f'GBM 모델의 MSE: {mse_gbm}')
```
&nbsp;
&nbsp;
### [ XGBoost ]

- *그래디언트 부스팅*을 기반으로 하는 고성능 앙상블 기법
- 원리
    - 초기 모델 학습
    - 잔여 오차 계산
    - 잔여 오차 학습
    - 모델 업데이트
    - 반복
- 장점
    - 병렬처리 → 학습 속도 향상
    - 조기 종료 → 성능 향상되지 않을 때 학습 조기 종료해 **과적합 방지**
    - 정규화 → **L1, L2 정규화**를 통해 모델 복잡도 조절
    - 유연성 → 다양한 손실함수, 평가 지표
- 코드

```python
import xgboost as xgb
from sklearn.metrics import mean_squared_error

# XGBoost 모델 생성
xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# 모델 학습
xgb_model.fit(X_train_scaled, y_train)

# 예측
y_pred_xgb = xgb_model.predict(X_test_scaled)

# 평가
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
print(f'XGBoost 모델의 MSE: {mse_xgb}')
```