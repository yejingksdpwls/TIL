## [ 비지도 학습 ]

### [ 차원축소 : PCA ]

![](https://velog.velcdn.com/images/yejingksdpwls/post/04915f04-f2d0-4337-b1cf-a82e4a52b770/image.png)

- 고차원 데이터를 저차원 데이터로 변환
- 데이터의 **분산**을 최대한 보존
- 데이터 주요 특징 추출
&nbsp;
- **작동원리**
    - 데이터 표준화
    - 공분산 행렬 계산
        - **공분산 행렬** : 데이터 각 특성 간의 공분산 나타내는 행렬
    - 고유값 및 고유벡터 계산
    - 주성분 선택
        - *고유값 클 수록* 해당 주성분이 분산 더 많이 설명
    - 데이터 변환
    &nbsp;
- 코드

```python
from sklearn.decomposition import PCA

# PCA 모델 생성
pca = PCA(n_components=0.95)  # 전체 분산의 95%를 설명하는 주성분 선택

# PCA 학습 및 변환
X_pca = pca.fit_transform(X_scaled)

# 변환된 데이터의 크기 확인
print(X_pca.shape)

# 주성분 확인
# 선택된 주성분의 수
print(f'선택된 주성분의 수: {pca.n_components_}')

# 각 주성분이 설명하는 분산 비율
print(f'각 주성분이 설명하는 분산 비율: {pca.explained_variance_ratio_}')

# 누적 분산 비율
print(f'누적 분산 비율: {pca.explained_variance_ratio_.cumsum()}')
```
&nbsp;
&nbsp;
### [ 차원축소 : t-SNE ]

- 고차원 데이터를 저차원으로 변환해 시각화하는 차원 축소 기법
- 데이터 포인트 간의 **유사성** 보존
- 고차원 데이터를 2차원/3차원 공간으로 변환
- **저차원으로 변환된 데이터의 유사성은 고차원 데이터의 유사성과 동일해야함**
&nbsp;
- *작동 원리*
    - 고차원 공간에서의 유사성 계산
    - 저차원 공간에서의 유사성 계산
    - *KL 발산 최소화* → 고차원 공간과 저차원 공간 간의 유사성 분포 차이를 KL 발산을 통해 최소화 ( 경사하강법 이용 )
    - 반복적 최적화
    &nbsp;
- 장점
    - 비선형 구조 탐지 가능
    - 클러스터 명확히 시각화
    - 고차원 데이터 시각화 가능
    &nbsp;
- 코드

```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

# t-SNE 모델 생성
tsne = TSNE(n_components=2, random_state=42)

# t-SNE 학습 및 변환
X_tsne = tsne.fit_transform(X_scaled)

# 변환된 데이터의 크기 확인
print(X_tsne.shape)
```
&nbsp;
&nbsp;
### [ 차원축소 : LDA ]

- 차원축소, 분류 동시 수행
- 클래스 간 분산 *최대화*
- 클래스 내 분산 *최소화*
&nbsp;
- *작동 원리*
    - 클래스 별 평균 계산
    - 클래스 내 분산 행렬 계산
    - 클래스 간 분산 행렬 계산
    - 고유값 및 고유벡터 계산
    - 선형 판별 축  선택
        - 고유값 **클 수록** 해당 선형 판별 축이 **클래스 간 분산 더 많이 설명**
    - 데이터 변환
    &nbsp;
- 코드

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# LDA 모델 생성
lda = LinearDiscriminantAnalysis(n_components=9)  # 클래스의 수 - 1 만큼의 선형 판별 축 선택

# LDA 학습 및 변환
X_lda = lda.fit_transform(X_scaled, y)

# 변환된 데이터의 크기 확인
print(X_lda.shape)
```